{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from keras.datasets import mnist, cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import random\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "seed_everything(22)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_stable(Z):\n",
    "    e_z = torch.exp(Z - torch.max(Z, dim=1, keepdim=True).values)\n",
    "    A = e_z / e_z.sum(dim=1, keepdim=True)\n",
    "    return A\n",
    "\n",
    "def softmax_loss(X, y, W):\n",
    "    A = softmax_stable(torch.mm(X, W))\n",
    "    # print(torch.log(A)[0][0])\n",
    "    return -torch.mean(torch.sum(y * torch.log(A), dim=1))\n",
    "\n",
    "def softmax_grad(X, y, W):\n",
    "    A = softmax_stable(torch.mm(X, W))\n",
    "    A = A - y\n",
    "    return torch.mm(X.T, A.to(torch.float32)) / X.shape[0]\n",
    "\n",
    "def softmax_fit(X, y, W, lr=0.05, epochs=500, tol=1e-5, batch_size=32):\n",
    "    W_old = torch.clone(W)\n",
    "    ep = 0\n",
    "    loss_hist = [softmax_loss(X, y, W).cpu().numpy()]\n",
    "    N = X.shape[0]\n",
    "    batches = int(np.ceil(N/batch_size))\n",
    "    \n",
    "    while ep < epochs:\n",
    "        ep += 1\n",
    "        mix_ids = torch.randperm(N)\n",
    "\n",
    "        for i in range(batches):\n",
    "            batch_ids = mix_ids[batch_size*i : min(batch_size*(i+1), N)]\n",
    "            X_batch, y_batch = X[batch_ids], y[batch_ids]\n",
    "            W = W - lr * softmax_grad(X_batch, y_batch, W)\n",
    "\n",
    "        loss_hist.append(softmax_loss(X, y, W).cpu().numpy())\n",
    "        print('Epoch:', ep, 'loss:', loss_hist[-1])        \n",
    "        # if torch.linalg.norm(W - W_old) / torch.numel(W) < tol:\n",
    "        #     print('Small change in weights')\n",
    "        #     break\n",
    "        \n",
    "        W_old = W\n",
    "    \n",
    "    return W, loss_hist\n",
    "\n",
    "def pred(W, X):\n",
    "    A = softmax_stable(torch.mm(X, W))\n",
    "    return torch.argmax(A, dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vnnews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đường dẫn\n",
    "train_x_path = 'data/vnnews/train_x.sav'\n",
    "train_y_path = 'data/vnnews/train_y.sav'\n",
    "test_x_path = 'data/vnnews/test_x.sav'\n",
    "test_y_path = 'data/vnnews/test_y.sav'\n",
    "\n",
    "# Load dữ liệu\n",
    "X_train = np.array(pickle.load(open(train_x_path, 'rb')))\n",
    "y_train = np.array(pickle.load(open(train_y_path, 'rb')))\n",
    "X_test = np.array(pickle.load(open(test_x_path, 'rb')))\n",
    "y_test = np.array(pickle.load(open(test_y_path, 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước vector input:  (14375, 79170)\n",
      "Kích thước vector test (12076, 79170)\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = Pipeline([\n",
    "    # Chuyển tập dữ liệu sang dạng vector count dựa trên một vocab chung (BoW), loại bỏ những word có tần suất xuất hiện > 0.8\n",
    "    ('vectorize', CountVectorizer(max_df=0.8, encoding='utf-16')),\n",
    "    \n",
    "    # Áp dụng TF-IDF để trích chọn đặc trưng (extract feature)\n",
    "    ('feature extracter', TfidfTransformer())\n",
    "])\n",
    "\n",
    "X_train = feature_extractor.fit_transform(X_train, y_train)\n",
    "X_test = feature_extractor.transform(X_test)\n",
    "print('Kích thước vector input: ', X_train.shape)\n",
    "print('Kích thước vector test', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32').toarray()\n",
    "X_test = X_test.astype('float32').toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Am nhac\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0.]\n",
      "Train samples: (14375, 79170)\n",
      "Test samples: (12076, 79170)\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0])\n",
    "le = OneHotEncoder()\n",
    "y_train = le.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "print(y_train[0])\n",
    "print('Train samples:', X_train.shape)\n",
    "print('Test samples:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).to(device)\n",
    "y_train = torch.from_numpy(y_train).to(device)\n",
    "X_test = torch.from_numpy(X_test).to(device)\n",
    "# y_test = torch.from_numpy(y_test).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_init = torch.randn(X_train.shape[1], y_train.shape[1]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 3.014058851588809\n",
      "Epoch: 2 loss: 2.472543764261707\n",
      "Epoch: 3 loss: 2.1073250327932205\n",
      "Epoch: 4 loss: 1.850442763061785\n",
      "Epoch: 5 loss: 1.6591570571464345\n",
      "Epoch: 6 loss: 1.509541540275511\n",
      "Epoch: 7 loss: 1.3883831995866711\n",
      "Epoch: 8 loss: 1.2880420384019335\n",
      "Epoch: 9 loss: 1.2031268249595666\n",
      "Epoch: 10 loss: 1.13028530666336\n",
      "Epoch: 11 loss: 1.0670587510514806\n",
      "Epoch: 12 loss: 1.011566279568772\n",
      "Epoch: 13 loss: 0.9625028589305465\n",
      "Epoch: 14 loss: 0.9186715742162581\n",
      "Epoch: 15 loss: 0.8792935268345723\n",
      "Epoch: 16 loss: 0.8436653865134781\n",
      "Epoch: 17 loss: 0.811286627400483\n",
      "Epoch: 18 loss: 0.7816374291888964\n",
      "Epoch: 19 loss: 0.7544422819634194\n",
      "Epoch: 20 loss: 0.7293775309431999\n",
      "Epoch: 21 loss: 0.7062047111649128\n",
      "Epoch: 22 loss: 0.684708939362258\n",
      "Epoch: 23 loss: 0.664662182510874\n",
      "Epoch: 24 loss: 0.6459641579084061\n",
      "Epoch: 25 loss: 0.6284471022878135\n",
      "Epoch: 26 loss: 0.6120195392137155\n",
      "Epoch: 27 loss: 0.5965745399464863\n",
      "Epoch: 28 loss: 0.5820051494672419\n",
      "Epoch: 29 loss: 0.5682592904308424\n",
      "Epoch: 30 loss: 0.5552325400767628\n",
      "Epoch: 31 loss: 0.5429173315022732\n",
      "Epoch: 32 loss: 0.531222185988793\n",
      "Epoch: 33 loss: 0.5200977107422795\n",
      "Epoch: 34 loss: 0.5095165044671358\n",
      "Epoch: 35 loss: 0.49941370103890154\n",
      "Epoch: 36 loss: 0.4897624687876136\n",
      "Epoch: 37 loss: 0.48050352340473895\n",
      "Epoch: 38 loss: 0.47165939821500014\n",
      "Epoch: 39 loss: 0.46318155284782203\n",
      "Epoch: 40 loss: 0.45503876880736344\n",
      "Epoch: 41 loss: 0.4472256579326304\n",
      "Epoch: 42 loss: 0.43970003720320444\n",
      "Epoch: 43 loss: 0.4324602014679303\n",
      "Epoch: 44 loss: 0.4254813477838175\n",
      "Epoch: 45 loss: 0.4187393958882893\n",
      "Epoch: 46 loss: 0.4122302545792304\n",
      "Epoch: 47 loss: 0.405935669535285\n",
      "Epoch: 48 loss: 0.39985663609423877\n",
      "Epoch: 49 loss: 0.3939706883859684\n",
      "Epoch: 50 loss: 0.3882717602628664\n",
      "Epoch: 51 loss: 0.3827522491818833\n",
      "Epoch: 52 loss: 0.3773947059443466\n",
      "Epoch: 53 loss: 0.37218749207156593\n",
      "Epoch: 54 loss: 0.3671338441321523\n",
      "Epoch: 55 loss: 0.36223940064347016\n",
      "Epoch: 56 loss: 0.3574699288615221\n",
      "Epoch: 57 loss: 0.3528319347557857\n",
      "Epoch: 58 loss: 0.3483118662045203\n",
      "Epoch: 59 loss: 0.3439106022884513\n",
      "Epoch: 60 loss: 0.33961720797399747\n",
      "Epoch: 61 loss: 0.33543778482236747\n",
      "Epoch: 62 loss: 0.33135702054055555\n",
      "Epoch: 63 loss: 0.327367529267179\n",
      "Epoch: 64 loss: 0.3234898101679094\n",
      "Epoch: 65 loss: 0.31969655959612414\n",
      "Epoch: 66 loss: 0.315999634619308\n",
      "Epoch: 67 loss: 0.31238628782712713\n",
      "Epoch: 68 loss: 0.3088518037931796\n",
      "Epoch: 69 loss: 0.305392921680547\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Nam 5\\2022.1\\CE Project\\Project\\softmax_svm.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Nam%205/2022.1/CE%20Project/Project/softmax_svm.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m lr \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Nam%205/2022.1/CE%20Project/Project/softmax_svm.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Nam%205/2022.1/CE%20Project/Project/softmax_svm.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m W, loss_hist \u001b[39m=\u001b[39m softmax_fit(X_train, y_train, W_init, lr\u001b[39m=\u001b[39;49mlr, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Nam%205/2022.1/CE%20Project/Project/softmax_svm.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTrain completed in \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start))\n",
      "\u001b[1;32me:\\Nam 5\\2022.1\\CE Project\\Project\\softmax_svm.ipynb Cell 13\u001b[0m in \u001b[0;36msoftmax_fit\u001b[1;34m(X, y, W, lr, epochs, tol, batch_size)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Nam%205/2022.1/CE%20Project/Project/softmax_svm.ipynb#X11sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     batch_ids \u001b[39m=\u001b[39m mix_ids[batch_size\u001b[39m*\u001b[39mi : \u001b[39mmin\u001b[39m(batch_size\u001b[39m*\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m), N)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Nam%205/2022.1/CE%20Project/Project/softmax_svm.ipynb#X11sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     X_batch, y_batch \u001b[39m=\u001b[39m X[batch_ids], y[batch_ids]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Nam%205/2022.1/CE%20Project/Project/softmax_svm.ipynb#X11sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     W \u001b[39m=\u001b[39m W \u001b[39m-\u001b[39m lr \u001b[39m*\u001b[39m softmax_grad(X_batch, y_batch, W)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Nam%205/2022.1/CE%20Project/Project/softmax_svm.ipynb#X11sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m loss_hist\u001b[39m.\u001b[39mappend(softmax_loss(X, y, W)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Nam%205/2022.1/CE%20Project/Project/softmax_svm.ipynb#X11sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch:\u001b[39m\u001b[39m'\u001b[39m, ep, \u001b[39m'\u001b[39m\u001b[39mloss:\u001b[39m\u001b[39m'\u001b[39m, loss_hist[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])        \n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 0.5\n",
    "start = time.time()\n",
    "W, loss_hist = softmax_fit(X_train, y_train, W_init, lr=lr, batch_size=32, epochs=200)\n",
    "print('Train completed in {}s'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31671a60cee805c34c73116577b485118ff3a75c458d3004d49632c19702ac60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
